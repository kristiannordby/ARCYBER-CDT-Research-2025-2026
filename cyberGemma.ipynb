{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9112d5ff-60e3-41f4-b407-2b7a209354a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76e80b80-604b-4a5a-a3a1-6e8196d7aa10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÅ Models will be saved to: /home/knordby/Documents/labeling/models\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "models_dir = \"/home/knordby/Documents/labeling/models\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "print(f\"\\nüìÅ Models will be saved to: {models_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502e273f-b249-4b55-9680-8b68ce8539bd",
   "metadata": {},
   "source": [
    "### Saving Embeddings...\n",
    "Here we load our embeddings and save them to this device. In the future (after this is done), we wont run these chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b963ac1-3ffa-4079-9a0d-fd87f0cb2267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/4] Loading embeddings...\n",
      "   Loading general_sample_200K embeddings...\n",
      "   Loaded 199793 embeddings from 200K dataset\n",
      "   Loading cyber_biased_sample_70K embeddings...\n",
      "   Loaded 62605 embeddings from 70K dataset\n",
      "   Total embeddings after merge: 262398\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[1/4] Loading embeddings...\")\n",
    "\n",
    "# Load 200K general embeddings\n",
    "print(\"   Loading general_sample_200K embeddings...\")\n",
    "with gzip.open('data/general_sample_200K_gemma_embedding.jsonl.gz', 'rt') as f:\n",
    "    _200k_embeddings = json.load(f)\n",
    "_200k_embeddings = {k.replace('.json', ''): v for k, v in _200k_embeddings.items()}\n",
    "print(f\"   Loaded {len(_200k_embeddings)} embeddings from 200K dataset\")\n",
    "\n",
    "# Load 70K cyber-biased embeddings\n",
    "print(\"   Loading cyber_biased_sample_70K embeddings...\")\n",
    "with gzip.open('data/cyber_biased_sample_70K_gemma_embedding.jsonl.gz', 'rt') as f:\n",
    "    _70k_embeddings = json.load(f)\n",
    "_70k_embeddings = {k.replace('.json', ''): v for k, v in _70k_embeddings.items()}\n",
    "print(f\"   Loaded {len(_70k_embeddings)} embeddings from 70K dataset\")\n",
    "\n",
    "# Merge embeddings\n",
    "gemma_embeddings = _70k_embeddings | _200k_embeddings\n",
    "print(f\"   Total embeddings after merge: {len(gemma_embeddings)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3757c871-07f6-4624-985d-180a7c93c472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Loading cyber_biased_sample_70K labels...\n",
      "   Loaded 47562 labels from 70K dataset\n",
      "   Loading general_sample_200K labels...\n",
      "   Loaded 160428 labels from 200K dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"   Loading cyber_biased_sample_70K labels...\")\n",
    "with open('data/gemma_cyber_biased_sample_70K.jsonl', 'r') as f:\n",
    "    cyber_biased_70k = json.load(f)\n",
    "print(f\"   Loaded {len(cyber_biased_70k)} labels from 70K dataset\")\n",
    "\n",
    "# Load 200K general labels\n",
    "print(\"   Loading general_sample_200K labels...\")\n",
    "with open(\"data/gemma-cyber-general_sample_200K.jsonl\", 'r') as f:\n",
    "    cyber_general_200k = json.load(f)\n",
    "print(f\"   Loaded {len(cyber_general_200k)} labels from 200K dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be56db9a-7871-43f6-97a7-af4ad8db101e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Population portion cyber-classified: 5.0%\n",
      "   Total labels after merge: 207990\n"
     ]
    }
   ],
   "source": [
    "# Calculate statistics for general labels\n",
    "general_labels = [1 if v == 'true' else 0 for v in cyber_general_200k.values()]\n",
    "pct_cyber = sum(general_labels) / len(general_labels)\n",
    "print(f\"   Population portion cyber-classified: {round(pct_cyber, 2) * 100}%\")\n",
    "\n",
    "# Merge labels\n",
    "cyber_labels = cyber_biased_70k | cyber_general_200k\n",
    "print(f\"   Total labels after merge: {len(cyber_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d6689b3-9b57-49ab-9cb6-04149c8b8ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cyber_embeddings_clean = {k:v for k,v in gemma_embeddings.items() if k in cyber_labels.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8401014f-e7b9-4c57-8a49-f13ccfe65f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cyber_embeddings = np.array([v for v in cyber_embeddings_clean.values()])\n",
    "ids = np.array([idx for idx in cyber_embeddings_clean.keys()])\n",
    "labels = np.array([1 if cyber_labels[str(idx)] == 'true' else 0 for idx in ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bd4525f-5def-4578-98ff-d4677c1c58d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\n",
    "    'cyber_gemma_embeddings_with_ids.npz',\n",
    "    embeddings=cyber_embeddings,\n",
    "    ids=ids,\n",
    "    labels=labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a15023a-094d-41f0-b837-af56a8ab4b67",
   "metadata": {},
   "source": [
    "### Load the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6ffa4bb-e8cb-400f-b277-ce3276e3baa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.86 s, sys: 443 ms, total: 6.31 s\n",
      "Wall time: 6.31 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.06934851, -0.03356409,  0.01137817, ...,  0.01780601,\n",
       "        -0.01562081, -0.02061358],\n",
       "       [-0.08027899,  0.03303464,  0.01234967, ...,  0.03866593,\n",
       "         0.06492707,  0.00011773],\n",
       "       [-0.03076481,  0.00835974,  0.01186978, ...,  0.02294729,\n",
       "        -0.00049745, -0.02872812],\n",
       "       ...,\n",
       "       [-0.09449892,  0.0213499 , -0.01632331, ..., -0.0328131 ,\n",
       "        -0.01384491,  0.04825141],\n",
       "       [-0.04347664,  0.03659043, -0.00044882, ..., -0.0002375 ,\n",
       "        -0.0075912 , -0.02220969],\n",
       "       [-0.04685944, -0.04274602,  0.00882755, ..., -0.01556782,\n",
       "         0.02710383, -0.05799818]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "data = np.load('datasets/cyber_gemma_embeddings_with_ids.npz')\n",
    "embeddings = data['embeddings']  # Shape: (N, embedding_dim)\n",
    "ids = data['ids']                 # Shape: (N,)\n",
    "labels = data['labels'] \n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b10d1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"english_ids.txt\", \"r\") as f:\n",
    "    english_ids = f.read().splitlines()\n",
    "english_ids_set = set(english_ids)\n",
    "with open(\"nonenglish_ids.txt\", \"r\") as f:\n",
    "    nonenglish_ids = f.read().splitlines()\n",
    "nonenglish_ids_set = set(nonenglish_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8f320f2-8b1f-4a95-bfa9-d080fa1f9456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['3c0972278d25b612e44337041daa8ec059116f41c02362730f8915bee659e618',\n",
       "       '0c7d596d47e125486e1d6150a7d7e66fc7bee5403b3cfc96dde2040ab3522bed',\n",
       "       'e13dc78430de551d9f82f5fa3a9129751e73d12a1761e4f3a701709c02e632ea',\n",
       "       ...,\n",
       "       '956f8a1f72f68848fea3a4381d2f6d6b65eea1e48473842efb13eeff48b99c08',\n",
       "       '23c946b5ff817da7c9ad896870c70795b06ada3bc8a97a8f1ef908250b766adc',\n",
       "       '75c31b57bbe658c85e3edbaa08cc592f28c608cafe9d12bf48fb39da7780f40e'],\n",
       "      dtype='<U64')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5917aa3b-051e-47f0-ab0b-fc66cd6a6c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6552349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_mask = np.array([id_ in english_ids_set for id_ in ids])\n",
    "nonenglish_mask = np.array([id_ in nonenglish_ids_set for id_ in ids])\n",
    "\n",
    "english_embeddings = embeddings[english_mask]\n",
    "nonenglish_embeddings = embeddings[nonenglish_mask]\n",
    "english_labels = labels[english_mask]\n",
    "nonenglish_labels = labels[nonenglish_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61ed063-23c2-4919-8a3b-1a296f067290",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85b8a065-adc1-4acd-ab7a-9976172f4512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/4] Preparing train/test split...\n",
      "x_train:  0.7999912733438054\n",
      "test size:  0.20000872665619468\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "print(\"\\n[3/4] Preparing train/test split...\")\n",
    "\n",
    "# x_train_ids,x_test_ids, y_train,y_test = train_test_split(ids, labels, train_size = 0.8, stratify = labels)\n",
    "english_xtrain, english_xtest, english_ytrain, english_ytest = train_test_split(\n",
    "    english_embeddings, english_labels, train_size=0.8, stratify=english_labels, random_state=42)\n",
    "nonenglish_xtrain, nonenglish_xtest, nonenglish_ytrain, nonenglish_ytest = train_test_split(\n",
    "    nonenglish_embeddings, nonenglish_labels, train_size=0.8, stratify=nonenglish_labels, random_state=42)\n",
    "\n",
    "x_train = np.concatenate([english_xtrain, nonenglish_xtrain], axis=0)\n",
    "y_train =  np.concatenate([english_ytrain, nonenglish_ytrain], axis=0)\n",
    "x_test = np.concatenate([english_xtest, nonenglish_xtest], axis=0)\n",
    "y_test =  np.concatenate([english_ytest, nonenglish_ytest], axis=0)\n",
    "\n",
    "print(\"x_train: \", len(x_train)/(len(x_train)+len(x_test)))\n",
    "print(\"test size: \", len(x_test)/(len(x_train)+len(x_test)))\n",
    "\n",
    "\n",
    "val_split_idx = int(len(x_train)*.85)\n",
    "x_val, y_val = x_train[val_split_idx:], y_train[val_split_idx:]\n",
    "x_train, y_train = x_train[:val_split_idx], y_train[:val_split_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83462fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3.5/4] Setting up WeightedRandomSampler for balanced training...\n",
      "Training class distribution:\n",
      "  Class 0: 103731 samples (95.1%)\n",
      "  Class 1: 5358 samples (4.9%)\n",
      "‚úì WeightedRandomSampler configured for ~50/50 class balance\n",
      "  Training will probabilistically sample to achieve balanced batches\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create WeightedRandomSampler for balanced training\n",
    "print(\"\\n[3.5/4] Setting up WeightedRandomSampler for balanced training...\")\n",
    "\n",
    "# Calculate class weights\n",
    "class_counts = np.bincount(y_train)\n",
    "print(f\"Training class distribution:\")\n",
    "print(f\"  Class 0: {class_counts[0]} samples ({class_counts[0]/len(y_train)*100:.1f}%)\")\n",
    "print(f\"  Class 1: {class_counts[1]} samples ({class_counts[1]/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "# Calculate sample weights (inverse frequency)\n",
    "sample_weights = 1.0 / class_counts[y_train]\n",
    "\n",
    "# Create sampler\n",
    "train_sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "print(f\"‚úì WeightedRandomSampler configured for ~50/50 class balance\")\n",
    "print(f\"  Training will probabilistically sample to achieve balanced batches\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030e8ac8-e22c-4144-a79f-f74d461d88ed",
   "metadata": {},
   "source": [
    "#### Dataset Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7888c8cd-df43-4378-8599-56c031dcb9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Dataset Statistics:\n",
      "   Training set shape: (109089, 768)\n",
      "   Test set shape: (32087, 768)\n",
      "   Embedding dimension: 768\n",
      "\n",
      "   Label Distribution:\n",
      "   ‚Ä¢ Training - Cyber: 5358 (4.9%)\n",
      "   ‚Ä¢ Training - Non-cyber: 103731 (95.1%)\n",
      "   ‚Ä¢ Test - Cyber: 1517 (4.7%)\n",
      "   ‚Ä¢ Test - Non-cyber: 30570 (95.3%)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nüìä Dataset Statistics:\")\n",
    "print(f\"   Training set shape: {x_train.shape}\")\n",
    "print(f\"   Test set shape: {x_test.shape}\")\n",
    "print(f\"   Embedding dimension: {x_train.shape[1]}\")\n",
    "print(f\"\\n   Label Distribution:\")\n",
    "print(f\"   ‚Ä¢ Training - Cyber: {sum(y_train)} ({sum(y_train)/len(y_train)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Training - Non-cyber: {len(y_train)-sum(y_train)} ({(len(y_train)-sum(y_train))/len(y_train)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Test - Cyber: {sum(y_test)} ({sum(y_test)/len(y_test)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Test - Non-cyber: {len(y_test)-sum(y_test)} ({(len(y_test)-sum(y_test))/len(y_test)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a6ba0a-274b-4de3-af75-66332a9ad399",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7020d7af-30dd-4f35-8028-a3eccfd9fa71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "======================================================================\n",
      "MODEL BUILT\n",
      "======================================================================\n",
      "Architecture: CyberClassifier\n",
      "Input dimension: 768\n",
      "Hidden layers: 512 -> 256 -> 128\n",
      "Output: 1 (binary classification)\n",
      "Total parameters: 561,409\n",
      "Trainable parameters: 561,409\n",
      "Device: cuda\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch_models import *\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Build model\n",
    "model, optimizer, criterion = build_model(\n",
    "    input_dim=x_train.shape[1],  # Auto-detect from your data\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ddf9bdd-4c58-4be8-a07c-dfdbabb9ff84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Using WeightedRandomSampler for balanced training\n",
      "======================================================================\n",
      "TRAINING\n",
      "======================================================================\n",
      "Epochs: 80\n",
      "Batch size: 512\n",
      "Training samples: 109089\n",
      "Validation samples: 19252\n",
      "Early stopping patience: 15\n",
      "======================================================================\n",
      "\n",
      "Epoch 1/80 - Time: 3.43s\n",
      "  Train - Loss: 0.1517, Acc: 0.9445, AUC: 0.9851\n",
      "  Val   - Loss: 0.1288, Acc: 0.9561, AUC: 0.9434, Precision: 0.4390, Recall: 0.6963\n",
      "  ‚úì Best model saved (AUC: 0.9434)\n",
      "\n",
      "Epoch 2/80 - Time: 2.69s\n",
      "  Train - Loss: 0.0475, Acc: 0.9864, AUC: 0.9975\n",
      "  Val   - Loss: 0.1276, Acc: 0.9646, AUC: 0.9400, Precision: 0.5156, Recall: 0.6285\n",
      "  No improvement (patience: 1/15)\n",
      "\n",
      "Epoch 3/80 - Time: 2.68s\n",
      "  Train - Loss: 0.0270, Acc: 0.9929, AUC: 0.9989\n",
      "  Val   - Loss: 0.1404, Acc: 0.9658, AUC: 0.9370, Precision: 0.5340, Recall: 0.5551\n",
      "  No improvement (patience: 2/15)\n",
      "\n",
      "Epoch 4/80 - Time: 2.68s\n",
      "  Train - Loss: 0.0189, Acc: 0.9950, AUC: 0.9994\n",
      "  Val   - Loss: 0.1566, Acc: 0.9679, AUC: 0.9369, Precision: 0.5710, Recall: 0.5113\n",
      "  No improvement (patience: 3/15)\n",
      "\n",
      "Epoch 5/80 - Time: 2.88s\n",
      "  Train - Loss: 0.0167, Acc: 0.9953, AUC: 0.9996\n",
      "  Val   - Loss: 0.1553, Acc: 0.9672, AUC: 0.9410, Precision: 0.5560, Recall: 0.5395\n",
      "  No improvement (patience: 4/15)\n",
      "\n",
      "Epoch 6/80 - Time: 2.67s\n",
      "  Train - Loss: 0.0101, Acc: 0.9973, AUC: 0.9998\n",
      "  Val   - Loss: 0.1715, Acc: 0.9693, AUC: 0.9360, Precision: 0.5973, Recall: 0.5071\n",
      "  No improvement (patience: 5/15)\n",
      "\n",
      "Epoch 7/80 - Time: 2.66s\n",
      "  Train - Loss: 0.0121, Acc: 0.9966, AUC: 0.9998\n",
      "  Val   - Loss: 0.1862, Acc: 0.9689, AUC: 0.9409, Precision: 0.5820, Recall: 0.5466\n",
      "  No improvement (patience: 6/15)\n",
      "\n",
      "Epoch 8/80 - Time: 2.81s\n",
      "  Train - Loss: 0.0046, Acc: 0.9990, AUC: 0.9999\n",
      "  Val   - Loss: 0.1780, Acc: 0.9702, AUC: 0.9326, Precision: 0.6232, Recall: 0.4788\n",
      "  No improvement (patience: 7/15)\n",
      "\n",
      "Epoch 9/80 - Time: 2.70s\n",
      "  Train - Loss: 0.0020, Acc: 0.9997, AUC: 1.0000\n",
      "  Val   - Loss: 0.1813, Acc: 0.9698, AUC: 0.9343, Precision: 0.6101, Recall: 0.4972\n",
      "  No improvement (patience: 8/15)\n",
      "\n",
      "Epoch 10/80 - Time: 2.66s\n",
      "  Train - Loss: 0.0018, Acc: 0.9996, AUC: 1.0000\n",
      "  Val   - Loss: 0.1892, Acc: 0.9704, AUC: 0.9351, Precision: 0.6271, Recall: 0.4845\n",
      "  No improvement (patience: 9/15)\n",
      "\n",
      "Epoch 11/80 - Time: 2.82s\n",
      "  Train - Loss: 0.0013, Acc: 0.9997, AUC: 1.0000\n",
      "  Val   - Loss: 0.1927, Acc: 0.9703, AUC: 0.9329, Precision: 0.6204, Recall: 0.4986\n",
      "  No improvement (patience: 10/15)\n",
      "\n",
      "Epoch 12/80 - Time: 2.69s\n",
      "  Train - Loss: 0.0009, Acc: 0.9999, AUC: 1.0000\n",
      "  Val   - Loss: 0.1887, Acc: 0.9686, AUC: 0.9342, Precision: 0.5828, Recall: 0.5169\n",
      "  No improvement (patience: 11/15)\n",
      "\n",
      "Epoch 13/80 - Time: 2.66s\n",
      "  Train - Loss: 0.0010, Acc: 0.9998, AUC: 1.0000\n",
      "  Val   - Loss: 0.2040, Acc: 0.9705, AUC: 0.9323, Precision: 0.6282, Recall: 0.4845\n",
      "  No improvement (patience: 12/15)\n",
      "\n",
      "Epoch 14/80 - Time: 2.82s\n",
      "  Train - Loss: 0.0008, Acc: 0.9999, AUC: 1.0000\n",
      "  Val   - Loss: 0.1990, Acc: 0.9691, AUC: 0.9321, Precision: 0.5904, Recall: 0.5212\n",
      "  No improvement (patience: 13/15)\n",
      "\n",
      "Epoch 15/80 - Time: 2.71s\n",
      "  Train - Loss: 0.0010, Acc: 0.9998, AUC: 1.0000\n",
      "  Val   - Loss: 0.2053, Acc: 0.9707, AUC: 0.9327, Precision: 0.6324, Recall: 0.4859\n",
      "  No improvement (patience: 14/15)\n",
      "\n",
      "Epoch 16/80 - Time: 2.87s\n",
      "  Train - Loss: 0.0003, Acc: 0.9999, AUC: 1.0000\n",
      "  Val   - Loss: 0.2025, Acc: 0.9702, AUC: 0.9313, Precision: 0.6218, Recall: 0.4831\n",
      "  No improvement (patience: 15/15)\n",
      "\n",
      "‚ö†Ô∏è  Early stopping triggered after 16 epochs\n",
      "\n",
      "======================================================================\n",
      "Loading best model...\n",
      "‚úÖ Best model loaded (AUC: 0.9434)\n",
      "üíæ Model saved to: /home/knordby/Documents/labeling/models/cyber_gemmaEmbeddings.pt\n",
      "‚è±Ô∏è  Total training time: 44.52s (0.74m)\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set save path\n",
    "model_path = '/home/knordby/Documents/labeling/models/cyber_gemmaEmbeddings.pt'\n",
    "\n",
    "# Train\n",
    "model, history = train_model(\n",
    "    model, optimizer, criterion,\n",
    "    x_train, y_train, x_val, y_val,\n",
    "    train_sampler=train_sampler,\n",
    "    device=device,\n",
    "    epochs=80,\n",
    "    batch_size=512,\n",
    "    model_path=model_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736256f5-1fa1-4b37-b4da-5f38e6a9e9d6",
   "metadata": {},
   "source": [
    "### Evaluate the Model's Performance Against the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1d5e970-c4b7-4218-bf64-c23414e4bc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìà CYBERSECURITY CLASSIFIER - FINAL TEST RESULTS\n",
      "======================================================================\n",
      "   Loss:      0.1536\n",
      "   Accuracy:  0.9456 (94.56%)\n",
      "   Precision: 0.4513\n",
      "   Recall:    0.6961\n",
      "   AUC:       0.9385\n",
      "   F1 Score:  0.5476\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Predicted\n",
      "                 Negative  Positive\n",
      "Actual Negative     29286      1284\n",
      "       Positive       461      1056\n",
      "\n",
      "Detailed Metrics:\n",
      "   True Positives:  1056\n",
      "   True Negatives:  29286\n",
      "   False Positives: 1284\n",
      "   False Negatives: 461\n",
      "   Specificity:     0.9580\n",
      "   NPV:             0.9845\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Non-Cyber     0.9845    0.9580    0.9711     30570\n",
      "       Cyber     0.4513    0.6961    0.5476      1517\n",
      "\n",
      "    accuracy                         0.9456     32087\n",
      "   macro avg     0.7179    0.8271    0.7593     32087\n",
      "weighted avg     0.9593    0.9456    0.9510     32087\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Test AUC: 0.9385\n"
     ]
    }
   ],
   "source": [
    "# Evaluate with detailed metrics\n",
    "y_pred_probs, metrics = evaluate_model(\n",
    "    model, x_test, y_test,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Access individual metrics if needed\n",
    "print(f\"Test AUC: {metrics['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248cccc3",
   "metadata": {},
   "source": [
    "#### English Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f646ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìà CYBERSECURITY CLASSIFIER - FINAL TEST RESULTS\n",
      "======================================================================\n",
      "   Loss:      0.2082\n",
      "   Accuracy:  0.9239 (92.39%)\n",
      "   Precision: 0.4744\n",
      "   Recall:    0.7350\n",
      "   AUC:       0.9291\n",
      "   F1 Score:  0.5766\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Predicted\n",
      "                 Negative  Positive\n",
      "Actual Negative      8264       544\n",
      "       Positive       177       491\n",
      "\n",
      "Detailed Metrics:\n",
      "   True Positives:  491\n",
      "   True Negatives:  8264\n",
      "   False Positives: 544\n",
      "   False Negatives: 177\n",
      "   Specificity:     0.9382\n",
      "   NPV:             0.9790\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Non-Cyber     0.9790    0.9382    0.9582      8808\n",
      "       Cyber     0.4744    0.7350    0.5766       668\n",
      "\n",
      "    accuracy                         0.9239      9476\n",
      "   macro avg     0.7267    0.8366    0.7674      9476\n",
      "weighted avg     0.9435    0.9239    0.9313      9476\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Test AUC: 0.9291\n"
     ]
    }
   ],
   "source": [
    "# Evaluate with detailed metrics\n",
    "y_pred_probs, metrics = evaluate_model(\n",
    "    model, english_xtest, english_ytest,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Access individual metrics if needed\n",
    "print(f\"Test AUC: {metrics['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cefd19",
   "metadata": {},
   "source": [
    "#### Non-English Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89c246d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìà CYBERSECURITY CLASSIFIER - FINAL TEST RESULTS\n",
      "======================================================================\n",
      "   Loss:      0.1308\n",
      "   Accuracy:  0.9547 (95.47%)\n",
      "   Precision: 0.4330\n",
      "   Recall:    0.6655\n",
      "   AUC:       0.9400\n",
      "   F1 Score:  0.5246\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Predicted\n",
      "                 Negative  Positive\n",
      "Actual Negative     21022       740\n",
      "       Positive       284       565\n",
      "\n",
      "Detailed Metrics:\n",
      "   True Positives:  565\n",
      "   True Negatives:  21022\n",
      "   False Positives: 740\n",
      "   False Negatives: 284\n",
      "   Specificity:     0.9660\n",
      "   NPV:             0.9867\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Non-Cyber     0.9867    0.9660    0.9762     21762\n",
      "       Cyber     0.4330    0.6655    0.5246       849\n",
      "\n",
      "    accuracy                         0.9547     22611\n",
      "   macro avg     0.7098    0.8157    0.7504     22611\n",
      "weighted avg     0.9659    0.9547    0.9593     22611\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Test AUC: 0.9400\n"
     ]
    }
   ],
   "source": [
    "# Evaluate with detailed metrics\n",
    "y_pred_probs, metrics = evaluate_model(\n",
    "    model, nonenglish_xtest, nonenglish_ytest,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Access individual metrics if needed\n",
    "print(f\"Test AUC: {metrics['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1ddbc6-372a-4d2c-9f04-e4f3987165db",
   "metadata": {},
   "source": [
    "### Push the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ef72e71-49af-4d6b-9b44-3f1dcb03bcf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PUSHING MODEL TO HUGGINGFACE\n",
      "======================================================================\n",
      "Repository: kristiangnordby/gemma_cyber\n",
      "Private: False\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Repository created/verified: kristiangnordby/gemma_cyber\n",
      "\n",
      "üìù Creating model card...\n",
      "‚öôÔ∏è  Saving configuration...\n",
      "üèóÔ∏è  Saving model architecture...\n",
      "üíæ Preparing model checkpoint...\n",
      "\n",
      "üì§ Uploading files to HuggingFace...\n",
      "  ‚úì Uploaded: README.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úì Uploaded: config.json\n",
      "  ‚úì Uploaded: model_architecture.py\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f12a4463dd04144b3966796d96cc8e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d71b5db0754d79969f7e1fa35c7845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úì Uploaded: model.pt\n",
      "\n",
      "======================================================================\n",
      "‚úÖ MODEL SUCCESSFULLY PUSHED TO HUGGINGFACE!\n",
      "======================================================================\n",
      "üîó View your model at: https://huggingface.co/kristiangnordby/gemma_cyber\n",
      "======================================================================\n",
      "\n",
      "Model available at: https://huggingface.co/kristiangnordby/gemma_cyber\n"
     ]
    }
   ],
   "source": [
    "from push_to_huggingface import push_to_huggingface\n",
    "\n",
    "with open(\"hf_token.txt\",'r') as f:\n",
    "    token = f.read()\n",
    "\n",
    "# Push your model (after training and evaluation)\n",
    "repo_url = push_to_huggingface(\n",
    "    model_path='/home/knordby/Documents/labeling/models/cyber_gemmaEmbeddings.pt',\n",
    "    repo_name='gemma_cyber',  # Choose your repo name\n",
    "    metrics=metrics,  # From evaluate_model()\n",
    "    input_dim=x_train.shape[1],  # Your embedding dimension\n",
    "    hf_token=token,  # Your token\n",
    "    private=False  # Set True if you want private repo\n",
    ")\n",
    "\n",
    "print(f\"Model available at: {repo_url}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vanilla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
